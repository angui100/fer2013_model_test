{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K9dYiWW1GZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9daf2c70-1614-45c0-d602-a0aacc9c0457"
      },
      "source": [
        "import sys, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To6qZhD81xGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64\n",
        "num_labels = 4\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "width, height = 48, 48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlK6BhjyQ5db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount your google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
        "#Following are labels\n",
        "#Calm 6\n",
        "#Surprise & Fear 5 & 2\n",
        "#Anger 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5J9XkWcV4_S",
        "colab_type": "code",
        "outputId": "fc3877f5-a829-4db5-a6a9-cf8ff200ee5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FnHICpsbNbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://drive.google.com/open?id=1OnveSEG0q5CwEQeZOW3QotcL_GK4UZ2l\n",
        "#/content/drive/My Drive/dataset/fer2013.csv\n",
        "root_path = '/content/drive/My Drive/dataset/fer2013.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXX-I0jU1yUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(root_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfsgviCx2W74",
        "colab_type": "code",
        "outputId": "bb191848-ad2b-481b-a7c4-5f75d16a054b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35882</th>\n",
              "      <td>6</td>\n",
              "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35883</th>\n",
              "      <td>3</td>\n",
              "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35884</th>\n",
              "      <td>0</td>\n",
              "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35885</th>\n",
              "      <td>3</td>\n",
              "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35886</th>\n",
              "      <td>2</td>\n",
              "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
              "      <td>PrivateTest</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion                                             pixels        Usage\n",
              "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
              "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
              "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
              "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
              "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J0L_kEyRhvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Remove parts of the data from the dataset to make small part training.\n",
        "indexNames_1 = data[ data['emotion'] == 1].index\n",
        "indexNames_3 = data[ data['emotion'] == 3].index\n",
        "indexNames_4 = data[ data['emotion'] == 4].index\n",
        " \n",
        "data.drop(indexNames_1 , inplace=True)\n",
        "#data.drop(indexNames_3 , inplace=True)\n",
        "#data.drop(indexNames_4 , inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvSoESb2K_UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(indexNames_3 , inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WoyuyWULFGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(indexNames_4 , inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_nzUOaEmKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e11e13dc-8bdb-4794-cbe4-e452f2006233"
      },
      "source": [
        "data.groupby('emotion').size()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "emotion\n",
              "0    4953\n",
              "2    5121\n",
              "5    4002\n",
              "6    6198\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dCM7giCFXEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#update 5 with 2\n",
        "#data.loc[data['emotion'] == 5, 'emotion'] = 2\n",
        "\n",
        "print(data['emotion']==1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCJy897U2eMz",
        "colab_type": "code",
        "outputId": "7fa8a3f1-0956-41b3-f182-8c22c436ac62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "pixels = data['pixels'].tolist() # 1\n",
        "\n",
        "faces = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')] # 2\n",
        "    if (len(face)) < 2304:\n",
        "      print(\"array length less than 2304\")\n",
        "      continue\n",
        "    face = np.asarray(face).reshape(width, height) # 3\n",
        "    \n",
        "    # There is an issue for normalizing images. Just comment out 4 and 5 lines until when I found the solution.\n",
        "    #face = face / 255.0 # 4\n",
        "    #face = cv2.resize(face.astype('uint8'), (width, height)) # 5\n",
        "    #face = face / 255.0\n",
        "    faces.append(face.astype('float32'))\n",
        "\n",
        "faces = np.asarray(faces)\n",
        "faces = np.expand_dims(faces, -1) # 6\n",
        "\n",
        "emotions = pd.get_dummies(data['emotion']).as_matrix() # 7\n",
        "print( emotions )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " ...\n",
            " [0 0 0 1]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ5_So1a5vLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ee9b5d9d-4811-474a-b849-eee2a7b3bb48"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)\n",
        "\n",
        "print(y_train)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1]\n",
            " [0 0 0 1]\n",
            " [0 1 0 0]\n",
            " ...\n",
            " [0 0 0 1]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007bzkFIUaGY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c290fbc-b79b-4d6a-f87f-26f3c08e54a7"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[ 32.]\n",
            "   [ 32.]\n",
            "   [ 34.]\n",
            "   ...\n",
            "   [ 24.]\n",
            "   [ 18.]\n",
            "   [ 19.]]\n",
            "\n",
            "  [[ 34.]\n",
            "   [ 32.]\n",
            "   [ 32.]\n",
            "   ...\n",
            "   [ 24.]\n",
            "   [ 18.]\n",
            "   [ 19.]]\n",
            "\n",
            "  [[ 32.]\n",
            "   [ 32.]\n",
            "   [ 33.]\n",
            "   ...\n",
            "   [ 23.]\n",
            "   [ 26.]\n",
            "   [ 24.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[237.]\n",
            "   [243.]\n",
            "   [246.]\n",
            "   ...\n",
            "   [199.]\n",
            "   [192.]\n",
            "   [183.]]\n",
            "\n",
            "  [[236.]\n",
            "   [241.]\n",
            "   [245.]\n",
            "   ...\n",
            "   [201.]\n",
            "   [197.]\n",
            "   [188.]]\n",
            "\n",
            "  [[237.]\n",
            "   [239.]\n",
            "   [245.]\n",
            "   ...\n",
            "   [198.]\n",
            "   [197.]\n",
            "   [191.]]]\n",
            "\n",
            "\n",
            " [[[ 71.]\n",
            "   [ 70.]\n",
            "   [ 72.]\n",
            "   ...\n",
            "   [103.]\n",
            "   [107.]\n",
            "   [106.]]\n",
            "\n",
            "  [[ 69.]\n",
            "   [ 70.]\n",
            "   [ 71.]\n",
            "   ...\n",
            "   [111.]\n",
            "   [108.]\n",
            "   [101.]]\n",
            "\n",
            "  [[ 68.]\n",
            "   [ 67.]\n",
            "   [ 87.]\n",
            "   ...\n",
            "   [112.]\n",
            "   [106.]\n",
            "   [ 94.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 68.]\n",
            "   [ 63.]\n",
            "   [ 63.]\n",
            "   ...\n",
            "   [ 58.]\n",
            "   [ 54.]\n",
            "   [ 53.]]\n",
            "\n",
            "  [[ 64.]\n",
            "   [ 64.]\n",
            "   [ 64.]\n",
            "   ...\n",
            "   [ 56.]\n",
            "   [ 53.]\n",
            "   [ 56.]]\n",
            "\n",
            "  [[ 64.]\n",
            "   [ 65.]\n",
            "   [ 62.]\n",
            "   ...\n",
            "   [ 56.]\n",
            "   [ 56.]\n",
            "   [ 58.]]]\n",
            "\n",
            "\n",
            " [[[219.]\n",
            "   [215.]\n",
            "   [207.]\n",
            "   ...\n",
            "   [  6.]\n",
            "   [  8.]\n",
            "   [ 13.]]\n",
            "\n",
            "  [[215.]\n",
            "   [206.]\n",
            "   [194.]\n",
            "   ...\n",
            "   [  5.]\n",
            "   [  9.]\n",
            "   [ 14.]]\n",
            "\n",
            "  [[209.]\n",
            "   [192.]\n",
            "   [152.]\n",
            "   ...\n",
            "   [  6.]\n",
            "   [  9.]\n",
            "   [ 17.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 12.]\n",
            "   [ 27.]\n",
            "   [ 37.]\n",
            "   ...\n",
            "   [ 51.]\n",
            "   [ 76.]\n",
            "   [103.]]\n",
            "\n",
            "  [[ 20.]\n",
            "   [ 18.]\n",
            "   [ 35.]\n",
            "   ...\n",
            "   [ 60.]\n",
            "   [ 80.]\n",
            "   [104.]]\n",
            "\n",
            "  [[ 40.]\n",
            "   [ 32.]\n",
            "   [ 26.]\n",
            "   ...\n",
            "   [ 64.]\n",
            "   [ 82.]\n",
            "   [ 96.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 44.]\n",
            "   [ 53.]\n",
            "   [ 80.]\n",
            "   ...\n",
            "   [ 52.]\n",
            "   [ 19.]\n",
            "   [  5.]]\n",
            "\n",
            "  [[ 39.]\n",
            "   [ 67.]\n",
            "   [ 84.]\n",
            "   ...\n",
            "   [ 48.]\n",
            "   [  4.]\n",
            "   [  8.]]\n",
            "\n",
            "  [[ 44.]\n",
            "   [ 69.]\n",
            "   [ 83.]\n",
            "   ...\n",
            "   [ 31.]\n",
            "   [  2.]\n",
            "   [  9.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  1.]\n",
            "   [  2.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [ 14.]\n",
            "   [ 11.]\n",
            "   [ 12.]]\n",
            "\n",
            "  [[  2.]\n",
            "   [  1.]\n",
            "   [ 38.]\n",
            "   ...\n",
            "   [  8.]\n",
            "   [  9.]\n",
            "   [ 14.]]\n",
            "\n",
            "  [[  2.]\n",
            "   [  0.]\n",
            "   [ 85.]\n",
            "   ...\n",
            "   [  3.]\n",
            "   [ 11.]\n",
            "   [ 14.]]]\n",
            "\n",
            "\n",
            " [[[  9.]\n",
            "   [ 13.]\n",
            "   [ 10.]\n",
            "   ...\n",
            "   [  7.]\n",
            "   [  7.]\n",
            "   [  7.]]\n",
            "\n",
            "  [[ 11.]\n",
            "   [ 13.]\n",
            "   [  1.]\n",
            "   ...\n",
            "   [  7.]\n",
            "   [  7.]\n",
            "   [  7.]]\n",
            "\n",
            "  [[ 12.]\n",
            "   [  8.]\n",
            "   [ 69.]\n",
            "   ...\n",
            "   [  6.]\n",
            "   [  8.]\n",
            "   [  8.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 72.]\n",
            "   [ 56.]\n",
            "   [ 34.]\n",
            "   ...\n",
            "   [  7.]\n",
            "   [  3.]\n",
            "   [  1.]]\n",
            "\n",
            "  [[ 55.]\n",
            "   [ 41.]\n",
            "   [ 22.]\n",
            "   ...\n",
            "   [ 10.]\n",
            "   [  3.]\n",
            "   [  1.]]\n",
            "\n",
            "  [[ 44.]\n",
            "   [ 27.]\n",
            "   [ 15.]\n",
            "   ...\n",
            "   [ 11.]\n",
            "   [  2.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[  8.]\n",
            "   [ 11.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  1.]\n",
            "   [  0.]\n",
            "   [  8.]]\n",
            "\n",
            "  [[ 11.]\n",
            "   [  6.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  1.]\n",
            "   [  1.]]\n",
            "\n",
            "  [[ 10.]\n",
            "   [  2.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  1.]\n",
            "   [  1.]\n",
            "   [  1.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [ 15.]\n",
            "   [  9.]\n",
            "   [  1.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtm02BJ67Ks_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(2*2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOtVcpzd7lYg",
        "colab_type": "code",
        "outputId": "c6b29bdc-e930-476e-ec7f-0e3688b43d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 46, 46, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 46, 46, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 23, 23, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 5,905,347\n",
            "Trainable params: 5,901,635\n",
            "Non-trainable params: 3,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZmI1mr7wmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWWYRF2O8MY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPYGzLWP8SCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorboard = TensorBoard(log_dir='/tmp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekoX8ouw8dua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4-zaDAZ8iPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELPATH = '/tmp/model.h5'\n",
        "checkpointer = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ7KjiSS89Xx",
        "colab_type": "code",
        "outputId": "f09f4170-76f1-4b42-e6d3-18e85186713b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(np.array(X_train), np.array(y_train),\n",
        "          batch_size=batch_size,\n",
        "          epochs=60,\n",
        "          verbose=1,\n",
        "          validation_data=(np.array(X_test), np.array(y_test)),\n",
        "          shuffle=True,\n",
        "          callbacks=[lr_reducer, tensorboard, checkpointer])\n",
        "#callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 16421 samples, validate on 2028 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/60\n",
            "16421/16421 [==============================] - 36s 2ms/step - loss: 1.6518 - acc: 0.2774 - val_loss: 1.3783 - val_acc: 0.2890\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37832, saving model to /tmp/model.h5\n",
            "Epoch 2/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.4014 - acc: 0.2988 - val_loss: 1.3895 - val_acc: 0.2899\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.37832\n",
            "Epoch 3/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.3684 - acc: 0.3274 - val_loss: 1.3220 - val_acc: 0.3664\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.37832 to 1.32201, saving model to /tmp/model.h5\n",
            "Epoch 4/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.3107 - acc: 0.3838 - val_loss: 1.4496 - val_acc: 0.2954\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.32201\n",
            "Epoch 5/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.2664 - acc: 0.4193 - val_loss: 1.6120 - val_acc: 0.3205\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.32201\n",
            "Epoch 6/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.2223 - acc: 0.4465 - val_loss: 1.3221 - val_acc: 0.3876\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.32201\n",
            "Epoch 7/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.1779 - acc: 0.4801 - val_loss: 1.2863 - val_acc: 0.3550\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.32201 to 1.28626, saving model to /tmp/model.h5\n",
            "Epoch 8/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.1484 - acc: 0.4908 - val_loss: 1.2697 - val_acc: 0.4300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.28626 to 1.26968, saving model to /tmp/model.h5\n",
            "Epoch 9/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.1109 - acc: 0.5168 - val_loss: 1.0798 - val_acc: 0.5187\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.26968 to 1.07977, saving model to /tmp/model.h5\n",
            "Epoch 10/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.0950 - acc: 0.5293 - val_loss: 1.0633 - val_acc: 0.5261\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.07977 to 1.06330, saving model to /tmp/model.h5\n",
            "Epoch 11/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.0680 - acc: 0.5357 - val_loss: 1.0754 - val_acc: 0.5178\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.06330\n",
            "Epoch 12/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.0415 - acc: 0.5511 - val_loss: 1.0518 - val_acc: 0.5242\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.06330 to 1.05176, saving model to /tmp/model.h5\n",
            "Epoch 13/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.0260 - acc: 0.5615 - val_loss: 1.0553 - val_acc: 0.5360\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.05176\n",
            "Epoch 14/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 1.0111 - acc: 0.5696 - val_loss: 1.3960 - val_acc: 0.2934\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.05176\n",
            "Epoch 15/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.9877 - acc: 0.5816 - val_loss: 1.0006 - val_acc: 0.5404\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.05176 to 1.00058, saving model to /tmp/model.h5\n",
            "Epoch 16/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.9752 - acc: 0.5893 - val_loss: 1.0188 - val_acc: 0.5508\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.00058\n",
            "Epoch 17/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.9650 - acc: 0.5941 - val_loss: 1.0312 - val_acc: 0.5390\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.00058\n",
            "Epoch 18/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.9393 - acc: 0.6088 - val_loss: 0.9236 - val_acc: 0.6040\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.00058 to 0.92361, saving model to /tmp/model.h5\n",
            "Epoch 19/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.9272 - acc: 0.6223 - val_loss: 1.0375 - val_acc: 0.5350\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.92361\n",
            "Epoch 20/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.8978 - acc: 0.6304 - val_loss: 0.9654 - val_acc: 0.5927\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.92361\n",
            "Epoch 21/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.8833 - acc: 0.6409 - val_loss: 0.9981 - val_acc: 0.5794\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.92361\n",
            "Epoch 22/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.8462 - acc: 0.6574 - val_loss: 1.0223 - val_acc: 0.5676\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.92361\n",
            "Epoch 23/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.8401 - acc: 0.6615 - val_loss: 0.9750 - val_acc: 0.6065\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.92361\n",
            "Epoch 24/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.8123 - acc: 0.6794 - val_loss: 0.8846 - val_acc: 0.6292\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.92361 to 0.88457, saving model to /tmp/model.h5\n",
            "Epoch 25/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.7937 - acc: 0.6865 - val_loss: 0.8953 - val_acc: 0.6267\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.88457\n",
            "Epoch 26/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.7583 - acc: 0.7012 - val_loss: 0.9151 - val_acc: 0.6410\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.88457\n",
            "Epoch 27/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.7365 - acc: 0.7136 - val_loss: 0.8874 - val_acc: 0.6509\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.88457\n",
            "Epoch 28/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.7194 - acc: 0.7226 - val_loss: 0.9136 - val_acc: 0.6366\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.88457\n",
            "Epoch 29/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.6790 - acc: 0.7414 - val_loss: 0.9591 - val_acc: 0.6312\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.88457\n",
            "Epoch 30/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.6510 - acc: 0.7551 - val_loss: 0.8499 - val_acc: 0.6691\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.88457 to 0.84994, saving model to /tmp/model.h5\n",
            "Epoch 31/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.6169 - acc: 0.7662 - val_loss: 0.8787 - val_acc: 0.6637\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.84994\n",
            "Epoch 32/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.5976 - acc: 0.7747 - val_loss: 0.9457 - val_acc: 0.6573\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.84994\n",
            "Epoch 33/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.5860 - acc: 0.7778 - val_loss: 0.9630 - val_acc: 0.6435\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.84994\n",
            "Epoch 34/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.5468 - acc: 0.7954 - val_loss: 0.9209 - val_acc: 0.6455\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.84994\n",
            "Epoch 35/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.5236 - acc: 0.8062 - val_loss: 0.9205 - val_acc: 0.6607\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.84994\n",
            "Epoch 36/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.4963 - acc: 0.8162 - val_loss: 0.9255 - val_acc: 0.6543\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.84994\n",
            "Epoch 37/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.4727 - acc: 0.8263 - val_loss: 0.9126 - val_acc: 0.6750\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.84994\n",
            "Epoch 38/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.4524 - acc: 0.8341 - val_loss: 0.9654 - val_acc: 0.6706\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.84994\n",
            "Epoch 39/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.4339 - acc: 0.8389 - val_loss: 0.9924 - val_acc: 0.6691\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.84994\n",
            "Epoch 40/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.4231 - acc: 0.8490 - val_loss: 0.9692 - val_acc: 0.6686\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.84994\n",
            "Epoch 41/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.3869 - acc: 0.8646 - val_loss: 1.0074 - val_acc: 0.6642\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.84994\n",
            "Epoch 42/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.3758 - acc: 0.8671 - val_loss: 1.0585 - val_acc: 0.6657\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.84994\n",
            "Epoch 43/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.3423 - acc: 0.8795 - val_loss: 1.0728 - val_acc: 0.6854\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.84994\n",
            "Epoch 44/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.3198 - acc: 0.8874 - val_loss: 1.0211 - val_acc: 0.6790\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.84994\n",
            "Epoch 45/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.3181 - acc: 0.8888 - val_loss: 1.0972 - val_acc: 0.6726\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.84994\n",
            "Epoch 46/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.3022 - acc: 0.8951 - val_loss: 1.1302 - val_acc: 0.6711\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.84994\n",
            "Epoch 47/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2924 - acc: 0.8989 - val_loss: 1.1442 - val_acc: 0.6721\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.84994\n",
            "Epoch 48/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2727 - acc: 0.9042 - val_loss: 1.2522 - val_acc: 0.6785\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.84994\n",
            "Epoch 49/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2601 - acc: 0.9099 - val_loss: 1.2695 - val_acc: 0.6573\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.84994\n",
            "Epoch 50/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2501 - acc: 0.9150 - val_loss: 1.1637 - val_acc: 0.6716\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.84994\n",
            "Epoch 51/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2410 - acc: 0.9185 - val_loss: 1.2083 - val_acc: 0.6731\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.84994\n",
            "Epoch 52/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2260 - acc: 0.9201 - val_loss: 1.2900 - val_acc: 0.6765\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.84994\n",
            "Epoch 53/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2163 - acc: 0.9253 - val_loss: 1.2338 - val_acc: 0.6691\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.84994\n",
            "Epoch 54/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.2023 - acc: 0.9309 - val_loss: 1.3195 - val_acc: 0.6686\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.84994\n",
            "Epoch 55/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.1966 - acc: 0.9353 - val_loss: 1.2843 - val_acc: 0.6780\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.84994\n",
            "Epoch 56/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.1946 - acc: 0.9341 - val_loss: 1.3191 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.84994\n",
            "Epoch 57/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.1812 - acc: 0.9386 - val_loss: 1.3746 - val_acc: 0.6790\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.84994\n",
            "Epoch 58/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.1707 - acc: 0.9431 - val_loss: 1.3630 - val_acc: 0.6849\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.84994\n",
            "Epoch 59/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.1625 - acc: 0.9456 - val_loss: 1.3602 - val_acc: 0.6869\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.84994\n",
            "Epoch 60/60\n",
            "16421/16421 [==============================] - 32s 2ms/step - loss: 0.1673 - acc: 0.9478 - val_loss: 1.3247 - val_acc: 0.6844\n",
            "\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.84994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdbaf18fbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPptlGIf9LTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "97519983-d19e-4355-b0cf-e424fcdf7cf8"
      },
      "source": [
        "scores = model.evaluate(np.array(X_test), np.array(y_test), batch_size=batch_size)\n",
        "print(\"Loss: \" + str(scores[0]))\n",
        "print(\"Accuracy: \" + str(scores[1]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2028/2028 [==============================] - 1s 628us/step\n",
            "Loss: 1.3247169223292574\n",
            "Accuracy: 0.6844181456039173\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}